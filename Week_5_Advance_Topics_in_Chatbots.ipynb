{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrJ6SG+SlseoArk5ejiove",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzad-r1zv1/Python_Course/blob/main/Week_5_Advance_Topics_in_Chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ====================================\n",
        "# Week 5: Advance Topics in Chatbots\n",
        "# ====================================\n",
        "\n"
      ],
      "metadata": {
        "id": "QMCgBWwMMaj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Goal\n",
        "Build a chatbot that uses GPT-style models to assist software testers and QA engineers with:\n",
        "\n",
        "*   Test case generation\n",
        "*   Writing PyTest or Robot Framework code\n",
        "*   Explaining testing concepts\n",
        "*   Suggesting test data or edge cases\n",
        "*   Answering common QA interview questions\n",
        "*   All built in Google Colab, optionally with a Gradio UI.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Tools & Libraries:\n",
        " - transformers library (Hugging Face)\n",
        " - Pre-trained model: gpt2 or flan-t5-base\n",
        " - Optionally: gradio for UI, datasets for finetuning (advanced)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Lesson Breakdown\n",
        "## Chatbot? How about a QABOT!! Let's make it more intellegent!\n",
        "**Key Learning Objective:** Use GenerativeLLM to build a more complex example.\n",
        "\n",
        "**Step 1:**\n",
        "Installs the required Python packages using pip:\n",
        "\n",
        "- transformers: Provides pre-trained deep learning models (like GPT-2, FLAN-T5) along with an easy-to-use pipeline interface.\n",
        "\n",
        "- gradio: Allows you to quickly build and launch a web interface to interact with your model in a user-friendly manner.\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "-If you want to use a different model or add additional packages (for example, torch or datasets), you can update the pip install line:\n",
        "\n"
      ],
      "metadata": {
        "id": "1Go5hddMVxmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Dependencies\n",
        "!pip install transformers gradio"
      ],
      "metadata": {
        "id": "sUhz_PcJ6ENE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Import Libraries and Initialize Models\n",
        "\n",
        "**What It Does:**\n",
        "\n",
        "**Imports:**\n",
        "\n",
        "- pipeline from transformers: This simplifies using pre-trained models by providing a high-level API.\n",
        "\n",
        "- set_seed from transformers: Ensures reproducibility by setting a random seed.\n",
        "\n",
        "- SequenceMatcher from difflib: Used later to compare two responses.\n",
        "\n",
        "\n",
        "**Model Initialization:**\n",
        "\n",
        "- GPT-2: Initialized using the \"text-generation\" pipeline. This model generates text based on a given prompt.\n",
        "\n",
        "- FLAN-T5: Initialized using the \"text2text-generation\" pipeline from Google's FLAN-T5 base model. It is optimized for instruction following and can provide responses based on a prompt.\n",
        "\n",
        "GPT-2 is excellent for generating creative, coherent responses but might require carefully crafted prompts to keep the focus narrow.  FLAN-T5 is designed to follow instructions and provide structured, relevant answers, though its responses might be more direct and less elaborative.\n",
        "We will use an ensemble method: Ensemble methods leverage these strengths, providing a flexible approach to generate an optimal answer for QA tasks.\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "- Switch Models: You might want to try different models by changing the model names, e.g., using \"gpt2-medium\" or \"microsoft/DialoGPT-medium\".\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "gpt2_bot = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "```\n",
        "**Adjust Seed:**\n",
        "\n",
        "Change set_seed(42) to any other integer to vary reproducibility."
      ],
      "metadata": {
        "id": "N9OET8CA6PB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Libraries and Initialize Models\n",
        "from transformers import pipeline, set_seed\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "set_seed(42)  # for reproducibility\n",
        "\n",
        "# GPT-2 Model for text-generation\n",
        "gpt2_bot = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# FLAN-T5 Model for text-to-text-generation (instruction following)\n",
        "flan_t5_bot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "#m2m_bot = pipeline(\"text2text-generation\", model=\"facebook/m2m100_418M\")"
      ],
      "metadata": {
        "id": "cL6q-TOCYlwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Define a Similarity Function\n",
        "**What It Does:**\n",
        "\n",
        "- This function calculates a similarity ratio between two strings using Python’s built-in SequenceMatcher.\n",
        "\n",
        "- The ratio is a float between 0 (completely different) and 1 (identical).\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "- Alternative Measures: You could replace this with a more sophisticated metric such as cosine similarity (if you convert strings into embeddings) or Levenshtein distance from libraries like python-Levenshtein.\n",
        "\n",
        "**Threshold Adjustment:**\n",
        "\n",
        "- When using the similarity score later, you might change the threshold (currently 0.8) to be more or less strict about considering responses as “similar.”"
      ],
      "metadata": {
        "id": "kjemvmxvMqps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define a Similarity Function to Compare Two Strings\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n"
      ],
      "metadata": {
        "id": "XUItlIHAMZ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define the Ensemble Chatbot Function\n",
        "**What It Does:**\n",
        "\n",
        "- **Prompt Creation**: Constructs two separate prompts from the user input—one for GPT-2 and one for FLAN-T5. This tailors the question for each model’s strengths.\n",
        "\n",
        "- **Response Generation**: Sends the tailored prompts to each model and retrieves their generated text. Cleans the text by stripping out the prompt from GPT-2’s response.\n",
        "\n",
        "- **Response Comparison & Selection**: Uses the similarity function to compute how similar the two responses are. If the similarity is above 0.8, it assumes both models gave a nearly identical answer and returns one. If they differ significantly, it concatenates both responses, providing additional detail.\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "- **Adjust Generation Parameters**: You can modify parameters like max_length, temperature, or top_p in the GPT-2 pipeline call. For example, to generate longer responses:\n",
        "\n",
        "```\n",
        "response1 = gpt2_bot(prompt_gpt, max_length=300, do_sample=True, temperature=0.6)[0][\"generated_text\"]`\n",
        "```\n",
        "\n",
        "- **Combine Responses Differently**: Instead of concatenating the responses, you could average similarity scores or use a weighted selection strategy.\n",
        "\n",
        "- **Custom Prompts**: Adjust the prompt templates to fit different domains. For example, for a medical QA bot, you might say:\n",
        "\n",
        "```\n",
        "prompt_gpt = f\"You are a helpful medical assistant. Answer this: {user_input}\\n\\n\"`\n",
        "```"
      ],
      "metadata": {
        "id": "bXLNiaJRmpcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the Ensemble Chatbot Function\n",
        "def generate_response(user_input):\n",
        "    # Prepare prompts for each model\n",
        "    prompt_gpt = f\"You are a helpful software tester, working for a global consultanting company. Concisely answer: {user_input}. Provide a concise explanation and an answer.\"\n",
        "    prompt_fl = f\"Help a software tester answer: {user_input}. Provide a concise explanation.\"\n",
        "\n",
        "    promt_extreme = f\"You are a highly experienced software testing and quality engineering expert. Your role is to assist QA professionals by providing clear, comprehensive, and technically detailed answers. When answering, please follow these guidelines: 1. **Restate the Question:**  Begin by briefly restating the question to ensure you understand the context.2. **Structured Response:**  Provide your answer using clear sections, such as: - **Definition/Overview:** Explain key terms and concepts. - **Detailed Explanation:** Break down the answer into steps, best practices, or key components. - **Code Examples:** If applicable, include sample code with clear comments. - **Pros and Cons:** (If comparing approaches) List advantages and drawbacks of each. 3. **Clarity and Conciseness:**  Ensure that your answer is clear and easy to understand. Use bullet points and numbered lists where appropriate to highlight key points. 4. **Technical Depth:**  Provide sufficient technical depth to be useful for both beginners and advanced professionals. Mention relevant tools, frameworks, and real-world scenarios if applicable. 5. **Professional Yet Approachable Tone:**    Maintain a professional tone while keeping the explanation accessible and engaging. 6. **Summary:**  Conclude with a bullet-point summary of the key takeaways. Now, please answer the following question as a QA assistant:  {user_input}\"\n",
        "\n",
        "    # Generate responses from both models\n",
        "    response1 = gpt2_bot(\n",
        "    prompt_gpt,\n",
        "    max_length=512,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    no_repeat_ngram_size=3  # This prevents repeating n-grams of 3 words or more\n",
        ")[0][\"generated_text\"]\n",
        "\n",
        "    response1 = response1.replace(prompt_gpt, \"\").strip()\n",
        "\n",
        "    response2 = flan_t5_bot(prompt_fl, max_length=512)[0][\"generated_text\"].strip()\n",
        "\n",
        "    #response3 = m2m_bot(prompt_gpt, max_length=512)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Compare the two responses using a similarity score\n",
        "    sim = similarity(response1, response2)\n",
        "\n",
        "    if sim > 0.8:\n",
        "        # The responses are very similar—return one of them.\n",
        "        optimal_response = response1\n",
        "    else:\n",
        "        # The responses differ—combine them for a richer output.\n",
        "        optimal_response = f\"\\n  GPT2 thinks:\\n{response1}\\n\\n  However, Flan says: \\n{response2}\"\n",
        "\n",
        "    return optimal_response"
      ],
      "metadata": {
        "id": "3BY61oGzm9zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Interactive Testing of the Ensemble Function\n",
        "\n",
        "- **What It Does**:\n",
        " - This code prompts the user to enter a question via standard input. It prints the response returned by the `generate_response` function.\n",
        "\n",
        "- **How to Modify**:\n",
        "  - **Input/Output Processing**: You might add logging to save user questions and model responses for further analysis. Or, integrate additional debugging outputs to compare the individual responses.\n",
        "\n",
        "- **Example queries:**\n",
        "  - \"Generate 5 negative test cases for email input\"\n",
        "  - \"Explain the difference between regression and retesting\"\n",
        " - \"Write a Robot Framework test case for a search bar\"\n",
        " - \"How do I calculate ROI of automation?\"\n",
        " - \"Write a PyTest function for user registration\""
      ],
      "metadata": {
        "id": "TqXsGxgbOy6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Test the Ensemble Response Function Interactively\n",
        "user_input = input(\"Enter your QA question: \")\n",
        "print(\"Bot:\", generate_response(user_input))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Vu8bYfYMx73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Launch a Gradio Interface\n",
        "**What It Does**:\n",
        "\n",
        " - **Gradio Setup**: Wraps the generate_response function into a Gradio interface to provide a web-based GUI.\n",
        "\n",
        "- **The interface includes**:\n",
        "\n",
        " -  A textbox input for the user question.\n",
        " - A text output for the chatbot response.\n",
        " - A title and description to explain the application.\n",
        "\n",
        "- **How to Modify**:\n",
        "\n",
        " - **UI Customization**: You can adjust the number of lines in the input textbox, change the title or description, or add more input components. For example, you could add a slider to adjust the generation temperature dynamically:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "inputs = [gr.Textbox(lines=2, placeholder=\"Enter your question...\"),\n",
        "          gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label=\"Temperature\")]\n",
        "```\n",
        "\n",
        "\n",
        "Additional Outputs:\n",
        "\n",
        "You might also display both model outputs side-by-side instead of combining them.\n",
        "\n",
        "Or, allow the user to select which model’s response to view."
      ],
      "metadata": {
        "id": "Hgpru4hHO6AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Launch a Gradio Interface for the Chatbot\n",
        "import gradio as gr\n",
        "\n",
        "gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a QA question, e.g. 'Write a PyTest test for login'\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"QA Assistant Chatbot Ensemble\",\n",
        "    description=\"This chatbot uses a combination of GPT-2 and FLAN-T5 to generate optimal responses for software testers and QA engineers.\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "SMgTe773N9tO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ===========\n",
        "# What's Next?\n",
        "# ===========\n",
        "\n",
        "1. ........\n",
        "\n",
        "\n",
        " **the sky is the limit!**\n",
        "\n",
        "\n",
        "\n",
        "# ========================\n",
        "# Reference/Further Deep Dive\n",
        "# ========================\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/transformers/index\n",
        "\n",
        "\n",
        "https://www.gradio.app/guides/quickstart\n"
      ],
      "metadata": {
        "id": "2A8UPSUQO-TW"
      }
    }
  ]
}