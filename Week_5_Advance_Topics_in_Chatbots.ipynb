{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeFvbUvY8WD5MNNXxwvnhE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzad-r1zv1/Python_Course/blob/main/Week_5_Advance_Topics_in_Chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ====================================\n",
        "# Week 5: Advance Topics in Chatbots\n",
        "# ====================================\n",
        "\n"
      ],
      "metadata": {
        "id": "QMCgBWwMMaj3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Goal\n",
        "Build a chatbot that uses GPT-style models to assist software testers and QA engineers with:\n",
        "\n",
        "*   Test case generation\n",
        "*   Writing PyTest or Robot Framework code\n",
        "*   Explaining testing concepts\n",
        "*   Suggesting test data or edge cases\n",
        "*   Answering common QA interview questions\n",
        "*   All built in Google Colab, optionally with a Gradio UI.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Tools & Libraries:\n",
        " - transformers library (Hugging Face)\n",
        " - Pre-trained model: gpt2 or flan-t5-base\n",
        " - Optionally: gradio for UI, datasets for finetuning (advanced)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#Lesson Breakdown\n",
        "## Chatbot? Let's make it more intellegent!\n",
        "**Key Learning Objective:** Use GenerativeLLM to build a more complex example.\n",
        "\n",
        "**Step 1:**\n",
        "Installs the required Python packages using pip:\n",
        "\n",
        "- transformers: Provides pre-trained deep learning models (like GPT-2, FLAN-T5) along with an easy-to-use pipeline interface.\n",
        "\n",
        "- gradio: Allows you to quickly build and launch a web interface to interact with your model in a user-friendly manner.\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "-If you want to use a different model or add additional packages (for example, torch or datasets), you can update the pip install line:\n",
        "\n"
      ],
      "metadata": {
        "id": "1Go5hddMVxmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Dependencies\n",
        "!pip install transformers gradio"
      ],
      "metadata": {
        "id": "sUhz_PcJ6ENE",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d381443-59b1-4316-df70-9dfb88270a5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lu6S8NAilmSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Import Libraries and Initialize Models\n",
        "\n",
        "**What It Does:**\n",
        "\n",
        "**Imports:**\n",
        "\n",
        "- pipeline from transformers: This simplifies using pre-trained models by providing a high-level API.\n",
        "\n",
        "- set_seed from transformers: Ensures reproducibility by setting a random seed.\n",
        "\n",
        "- SequenceMatcher from difflib: Used later to compare two responses.\n",
        "\n",
        "\n",
        "**Model Initialization:**\n",
        "\n",
        "- GPT-2: Initialized using the \"text-generation\" pipeline. This model generates text based on a given prompt.\n",
        "\n",
        "- FLAN-T5: Initialized using the \"text2text-generation\" pipeline from Google's FLAN-T5 base model. It is optimized for instruction following and can provide responses based on a prompt.\n",
        "\n",
        "GPT-2 is excellent for generating creative, coherent responses but might require carefully crafted prompts to keep the focus narrow.  FLAN-T5 is designed to follow instructions and provide structured, relevant answers, though its responses might be more direct and less elaborative.\n",
        "We will use an ensemble method: Ensemble methods leverage these strengths, providing a flexible approach to generate an optimal answer for QA tasks.\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "- Switch Models: You might want to try different models by changing the model names, e.g., using \"gpt2-medium\" or \"microsoft/DialoGPT-medium\".\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "gpt2_bot = pipeline(\"text-generation\", model=\"gpt2-medium\")\n",
        "```\n",
        "**Adjust Seed:**\n",
        "\n",
        "Change set_seed(42) to any other integer to vary reproducibility."
      ],
      "metadata": {
        "id": "N9OET8CA6PB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import Libraries and Initialize Models\n",
        "from transformers import pipeline, set_seed\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "set_seed(42)  # for reproducibility\n",
        "\n",
        "# GPT-2 Model for text-generation\n",
        "gpt2_bot = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# FLAN-T5 Model for text-to-text-generation (instruction following)\n",
        "flan_t5_bot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "cL6q-TOCYlwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5438cec0-08a3-4d08-ae61-5cfdfc6226d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Define a Similarity Function\n",
        "**What It Does:**\n",
        "\n",
        "- This function calculates a similarity ratio between two strings using Python’s built-in SequenceMatcher.\n",
        "\n",
        "- The ratio is a float between 0 (completely different) and 1 (identical).\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "- Alternative Measures: You could replace this with a more sophisticated metric such as cosine similarity (if you convert strings into embeddings) or Levenshtein distance from libraries like python-Levenshtein.\n",
        "\n",
        "**Threshold Adjustment:**\n",
        "\n",
        "- When using the similarity score later, you might change the threshold (currently 0.8) to be more or less strict about considering responses as “similar.”"
      ],
      "metadata": {
        "id": "kjemvmxvMqps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define a Similarity Function to Compare Two Strings\n",
        "def similarity(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n"
      ],
      "metadata": {
        "id": "XUItlIHAMZ89"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define the Ensemble Chatbot Function\n",
        "**What It Does:**\n",
        "\n",
        "- **Prompt Creation**: Constructs two separate prompts from the user input—one for GPT-2 and one for FLAN-T5. This tailors the question for each model’s strengths.\n",
        "\n",
        "- **Response Generation**: Sends the tailored prompts to each model and retrieves their generated text. Cleans the text by stripping out the prompt from GPT-2’s response.\n",
        "\n",
        "- **Response Comparison & Selection**: Uses the similarity function to compute how similar the two responses are. If the similarity is above 0.8, it assumes both models gave a nearly identical answer and returns one. If they differ significantly, it concatenates both responses, providing additional detail.\n",
        "\n",
        "**How to Modify:**\n",
        "\n",
        "- **Adjust Generation Parameters**: You can modify parameters like max_length, temperature, or top_p in the GPT-2 pipeline call. For example, to generate longer responses:\n",
        "\n",
        "```\n",
        "response1 = gpt2_bot(prompt_gpt, max_length=300, do_sample=True, temperature=0.6)[0][\"generated_text\"]`\n",
        "```\n",
        "\n",
        "- **Combine Responses Differently**: Instead of concatenating the responses, you could average similarity scores or use a weighted selection strategy.\n",
        "\n",
        "- **Custom Prompts**: Adjust the prompt templates to fit different domains. For example, for a medical QA bot, you might say:\n",
        "\n",
        "```\n",
        "prompt_gpt = f\"You are a helpful medical assistant. Answer this: {user_input}\\n\\n\"`\n",
        "```"
      ],
      "metadata": {
        "id": "bXLNiaJRmpcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the Ensemble Chatbot Function\n",
        "def generate_response(user_input):\n",
        "    # Prepare prompts for each model\n",
        "    prompt_gpt = f\"You are a helpful software QA assistant. Answer this: {user_input}\\n\\n\"\n",
        "    prompt_fl = f\"Help a QA tester: {user_input}\"\n",
        "\n",
        "    # Generate responses from both models\n",
        "    response1 = gpt2_bot(prompt_gpt, max_length=1024, do_sample=True, temperature=0.7, top_p=0.9)[0][\"generated_text\"]\n",
        "    response1 = response1.replace(prompt_gpt, \"\").strip()\n",
        "\n",
        "    response2 = flan_t5_bot(prompt_fl, max_length=1024)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Compare the two responses using a similarity score\n",
        "    sim = similarity(response1, response2)\n",
        "\n",
        "    if sim > 0.8:\n",
        "        # The responses are very similar—return one of them.\n",
        "        optimal_response = response1\n",
        "    else:\n",
        "        # The responses differ—combine them for a richer output.\n",
        "        optimal_response = f\"{response1}\\n\\nAdditionally:\\n{response2}\"\n",
        "\n",
        "    return optimal_response"
      ],
      "metadata": {
        "id": "3BY61oGzm9zO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Interactive Testing of the Ensemble Function\n",
        "\n",
        "- **What It Does**:\n",
        " - This code prompts the user to enter a question via standard input. It prints the response returned by the `generate_response` function.\n",
        "\n",
        "- **How to Modify**:\n",
        "  - **Input/Output Processing**: You might add logging to save user questions and model responses for further analysis. Or, integrate additional debugging outputs to compare the individual responses."
      ],
      "metadata": {
        "id": "TqXsGxgbOy6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Test the Ensemble Response Function Interactively\n",
        "user_input = input(\"Enter your QA question: \")\n",
        "print(\"Bot:\", generate_response(user_input))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Vu8bYfYMx73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11af843-a372-4940-b4de-48e7e683aa58"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your QA question: what is a test case?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: A test case is a collection of questions, questions that you can answer or ask about your software. A test case is an automated process, so it can be automated and you can answer them all. The test case is a test case for the software. A test case is an automated process, so it can be automated and you can answer them all. The test case is a test case for the software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection of questions, questions that you can answer or ask about your software. Answer this: what is a test case for?\n",
            "\n",
            "\n",
            "A test case is a collection of questions, questions that you can answer or ask about your software. A test case is a collection\n",
            "\n",
            "Additionally:\n",
            "A test case is a test that tests a software product. A test case is a test that tests a software product. A test case is a test that tests a software product.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Launch a Gradio Interface\n",
        "**What It Does**:\n",
        "\n",
        " - **Gradio Setup**: Wraps the generate_response function into a Gradio interface to provide a web-based GUI.\n",
        "\n",
        "- **The interface includes**:\n",
        "\n",
        " -  A textbox input for the user question.\n",
        " - A text output for the chatbot response.\n",
        " - A title and description to explain the application.\n",
        "\n",
        "- **How to Modify**:\n",
        "\n",
        " - **UI Customization**: You can adjust the number of lines in the input textbox, change the title or description, or add more input components. For example, you could add a slider to adjust the generation temperature dynamically:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "inputs = [gr.Textbox(lines=2, placeholder=\"Enter your question...\"),\n",
        "          gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.1, label=\"Temperature\")]\n",
        "```\n",
        "\n",
        "\n",
        "Additional Outputs:\n",
        "\n",
        "You might also display both model outputs side-by-side instead of combining them.\n",
        "\n",
        "Or, allow the user to select which model’s response to view."
      ],
      "metadata": {
        "id": "Hgpru4hHO6AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Launch a Gradio Interface for the Chatbot\n",
        "import gradio as gr\n",
        "\n",
        "gr.Interface(\n",
        "    fn=generate_response,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a QA question, e.g. 'Write a PyTest test for login'\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"QA Assistant Chatbot Ensemble\",\n",
        "    description=\"This chatbot uses a combination of GPT-2 and FLAN-T5 to generate optimal responses for software testers and QA engineers.\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "SMgTe773N9tO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ===========\n",
        "# What's Next?\n",
        "# ===========\n",
        "\n",
        "1. ....\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2A8UPSUQO-TW"
      }
    }
  ]
}